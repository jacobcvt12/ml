---
title: "Classification and Regression Trees"
author: "Jacob Carey"
date: \today
header-includes:
   - \usepackage{subfigure}
output: 
  beamer_presentation:
    theme: "Szeged"
    fig_caption: false
    slide_level: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(dplyr)
library(tidyr)
library(ggplot2)
```

# Reminders

## Latent function

- Consider $y_i \sim \text{Normal}(\mathbf{f(X_i)}, \sigma^2)$
- Interest is in this *latent function* in order to understand the *data generating process*
    - Aside: we require the latent function to be $f(X_i) = X_i \cdot \beta$ for linear regression

## R example of polynomial DGP

```{r dgp, echo=TRUE}
set.seed(1)
b <- c(-1, 4, 3) # coefficients
n <- 1000 # number of observations
x <- runif(n, -5, 5) # simulate random input data
f <- cbind(1, x, x ^ 2) %*% b # construct func.
y <- rnorm(n, f) # simulate with variance 5
```

## Visualization of polynomial DGP

```{r dgp-viz}
d <- data_frame(x=x, f=f[, 1], y=y)
ggplot(d, aes(x=x, y=y)) +
    geom_point(colour="orange", alpha=0.5) +
    geom_line(aes(y=f), colour="blue", size=1.5) +
    labs(title="Polynomial Data Generating Process",
         caption="Observed data as points\nLatent function as line")
```

# CART

## Terminology

```{r term-example}
library(tree)
mod.lm <- lm(y ~ x)
mod.tree <- tree(y ~ x)

plot(mod.tree)
text(mod.tree, pretty=0)
```

## Partitions

\begin{figure}
\hfill
\subfigure{\includegraphics[width=5cm]{hitter-tree.png}}
\hfill
\subfigure{\includegraphics[width=5cm]{partition.png}}
\hfill
\caption{Predicting Log Baseball Salary}
\end{figure}

## Algorithm

- Goal is to find partitions $R_1, ..., R_J$ such that we minimize the RSS
  $$\sum_{j=1}^J\sum_{i \in R_j} (y_i - \hat{y}_{R_j})^2$$
- Perform binary splitting by selecting an $X_j$ and cutpoint $s$ such that arising regions $R_1$ and $R_2$ and values $\hat{y}_{R_1}$ and $\hat{y}_{R_2}$ minimize the RSS
- $\hat{y}_{R_j}$ is calculated as mean response for observations in region $R_j$
- Continue splitting each region until stopping point is reached

## Hyperparameters and Tuning

- Typically, tree constructed in this fashion will be overfit
- Next step involves *pruning* the tree to obtain a subtree
- Resulting subtree will have less variance at the cost of some bias
    - Aside: the number of terminal nodes is a hyperparameter in CART

# Examples

## Training
```{r training, echo=TRUE}
training <- sample(0:1, floor(n * 0.7), TRUE)

library(tree)
mod.tree <- tree(y ~ x, subset=training == 1)

d <- data.frame(x[training == 0], y[training == 0])
names(d) <- c("x", "y")

pred.tree <- predict(mod.tree, d)
```

## Prediction

```{r example}
d <- data_frame(x=d$x, 
                observed=d$y,
                tree=pred.tree) %>%
    gather(key, value, observed:tree)

ggplot(d, aes(x, value)) + 
    geom_line(aes(colour=key,
                  linetype=key))
```

## Tuning

```{r tuning}
cv.mod <- cv.tree(mod.tree)
```
