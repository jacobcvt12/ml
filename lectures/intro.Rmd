---
title: "Introduction to Machine Learning"
author: "Jacob Carey"
date: \today
output: 
  beamer_presentation:
    theme: "Szeged"
    fig_caption: false
    slide_level: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(dplyr)
library(tidyr)
library(ggplot2)
```

# Background

## Mean/Variance vs Probability

- WLOG we consider the error terms to be either Gaussian or Binomail
- In other words, we either allow the outcome to be on the $(-\infty, \infty)$ or in ${0, 1}$.
- For the gaussian errors, we will be interested in the mean (and less so, the variance).
- For the binomial errors, we will *typically* investigate the **probability** of the outcome (even though we observe the binary outcomes)

## Notation

- For the continuous, unbounded outcome case, we will denote the outcome as $$y \sim \text{Gaussian}(\mu, \sigma^2)$$
- Here $\mu$ indicates the mean and $\sigma^2$ indicates the variance
- For the binary outcome case, we will denote the outcome as $$y \sim \text{Binomial}(p)$$
- Here $p$ indicates the probability of observing a "1" and conversely $1-p$ is the probability of observing a "0"

## Multiple observations

- In general, we have more than one observation
- For the continuous case, we are interested in understanding $y_i \sim \text{Normal}(\mu_i, \sigma^2)$
- And in the binary case, we are interested in understanding $y_i \sim \text{Binomial}(p_i)$

## Latent function

- We consider $y_i \sim \text{Normal}(f(X_i), \sigma^2)$
- We seek to understand this latent function to understand the data generating process
- Aside: we require the latent function to be $f(X_i) = X_i \cdot \beta$ for linear regression

## A note about the variance

- More simply, we typically consider the $\sigma^2$ to be constant across all observations
- Known as homoscedastic
- More sophisticated models (which may be considered later) allow for varying $\sigma^2_i$'s
- Known as heteroscedastic

## Final note

- In many of the machine learning models we investigate, we practically ignore the error terms. 
- We often limit ourselves to making inference on the latent function $f(X_i)$.

# Training, testing, hyperparameters, and philosophy

## Training/Testing

- For fitting a model, we typically split our data into a *training* and *testing* sets.
- The model is fit to the training data, and then predictions are made on the testing set
- This provides us with a "practical" sense of how well our model will perform in making predictions
- We expect the model to perform better on the training data than the (held out) test data
- Examining accuracy in the training data only will generally lead us to be overconfident in our model

## Model Selection

- We typically use "Mean Squared Error" (MSE) to calculate accuracy
- Other measures are available as well
- The MSE as calculated on the testing is a good method for model selection when prediction accuracy is the primary concern
- For example, if testing MSE for model 1 is greater than testing MSE for model 2, then model 1 should be used for making predictions

## Hyperparameters

- Many models have so-called "hyperparameters" that do not have a closed form solution or an optimization routine for solving
- Solution of these parameters is *typically* performed by cross-validation on a grid of values within the training set
- Suppose that model A has hyperparameter $\eta$ that must be tuned, and we suppose that it can take values of either 0.1 or 0.01.
- The training data is typically broken further into sub-training and sub-testing sets, and the optimal hyperparameter is chosen based on the sub-testing MSE

## Philosophy

- For now, we will limit ourselves to frequentist models 
- The "common" techniques in ML our "frequently" frequentist
- However, Bayesian ML techniques have many advantages, such as a "nicer" method for hyperparameter selection, incorporation of "prior" information, and more sophisticated models that may not exist within a frequentist paradigm
- Good to be aware of these more advanced approaches, even if we aren't covering them now

# Models

## Important!

> All models are wrong; some are useful

- George Box

## Models intended to be covered

- Boosting/Bagging
- Ridge Regression/Lasso
- Support Vector Machines
- Clustering Techniques

## Models for future consideration

- Neural Networks
- Bayesian Nonparametrics

## Flexibility vs Interpretability

![ISLR Graph](interpretability-flexibility.png)
